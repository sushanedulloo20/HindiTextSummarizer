{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-24T17:11:36.709718Z","iopub.status.busy":"2024-04-24T17:11:36.708571Z","iopub.status.idle":"2024-04-24T17:11:36.716419Z","shell.execute_reply":"2024-04-24T17:11:36.715315Z","shell.execute_reply.started":"2024-04-24T17:11:36.709677Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["# FineTuning mT5 for summarization Task"]},{"cell_type":"markdown","metadata":{},"source":["For finetuning we have used the summarization tutorial given by hugging face:\n","https://huggingface.co/docs/transformers/en/tasks/summarization"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T21:19:33.505016Z","iopub.status.busy":"2024-04-24T21:19:33.504652Z","iopub.status.idle":"2024-04-24T21:19:51.651633Z","shell.execute_reply":"2024-04-24T21:19:51.650542Z","shell.execute_reply.started":"2024-04-24T21:19:33.504987Z"},"trusted":true},"outputs":[],"source":["!pip install transformers datasets evaluate rouge_score"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:50:49.066620Z","iopub.status.busy":"2024-04-25T12:50:49.065756Z","iopub.status.idle":"2024-04-25T12:50:49.091689Z","shell.execute_reply":"2024-04-25T12:50:49.090829Z","shell.execute_reply.started":"2024-04-25T12:50:49.066589Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"234915e5b26e456c87541c547ac65f8e","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:44:31.817605Z","iopub.status.busy":"2024-04-25T12:44:31.816758Z","iopub.status.idle":"2024-04-25T12:44:34.170717Z","shell.execute_reply":"2024-04-25T12:44:34.169700Z","shell.execute_reply.started":"2024-04-25T12:44:31.817570Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","train=pd.read_csv(\"/kaggle/input/hindid/hindi_train.csv\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:44:48.343692Z","iopub.status.busy":"2024-04-25T07:44:48.342993Z","iopub.status.idle":"2024-04-25T07:44:48.364562Z","shell.execute_reply":"2024-04-25T07:44:48.363482Z","shell.execute_reply.started":"2024-04-25T07:44:48.343662Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>Heading</th>\n","      <th>Summary</th>\n","      <th>Article</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>hindi_2023_train_0</td>\n","      <td>गला दबाकर हत्या की; बॉडी बोरे में भरी, लोकल मा...</td>\n","      <td>Kerala Minor Girl Rape Case - केरल के एर्नाकुल...</td>\n","      <td>केरल के एर्नाकुलम जिले में 5 साल की बच्ची से र...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>hindi_2023_train_1</td>\n","      <td>तेलंगाना में 18 की जान गई; जम्मू-कश्मीर में बा...</td>\n","      <td>इस साल मानसून सीजन में कई राज्यों में भारी तबा...</td>\n","      <td>मानसून सीजन में हुई भारी बारिश ने कई राज्यों म...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>hindi_2023_train_2</td>\n","      <td>राजस्थान सरकार बनाएगी कर्ज राहत आयोग, कोर्ट के...</td>\n","      <td>चुनावी साल में राजस्थान सरकार किसानों को लुभान...</td>\n","      <td>चुनावी साल में राजस्थान सरकार किसानों को लुभान...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>hindi_2023_train_3</td>\n","      <td>3 से 7 अगस्त तक कर सकेंगे अप्लाय, प्राइस बैंड ...</td>\n","      <td>Non-banking lender SBFC Finance's initial publ...</td>\n","      <td>नॉन बैंकिग फाइनेंस कंपनी 'SBFC फाइनेंस लिमिटेड...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>hindi_2023_train_4</td>\n","      <td>डाइनिंग टेबल पर कुकर-कड़ाही न सजाएं, चीन के खा...</td>\n","      <td>स्वाद खाने की बुनियाद है। लेकिन अगर खाना सुंदर...</td>\n","      <td>स्वाद खाने की बुनियाद है। लेकिन अगर खाना सुंदर...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   Id                                            Heading  \\\n","0  hindi_2023_train_0  गला दबाकर हत्या की; बॉडी बोरे में भरी, लोकल मा...   \n","1  hindi_2023_train_1  तेलंगाना में 18 की जान गई; जम्मू-कश्मीर में बा...   \n","2  hindi_2023_train_2  राजस्थान सरकार बनाएगी कर्ज राहत आयोग, कोर्ट के...   \n","3  hindi_2023_train_3  3 से 7 अगस्त तक कर सकेंगे अप्लाय, प्राइस बैंड ...   \n","4  hindi_2023_train_4  डाइनिंग टेबल पर कुकर-कड़ाही न सजाएं, चीन के खा...   \n","\n","                                             Summary  \\\n","0  Kerala Minor Girl Rape Case - केरल के एर्नाकुल...   \n","1  इस साल मानसून सीजन में कई राज्यों में भारी तबा...   \n","2  चुनावी साल में राजस्थान सरकार किसानों को लुभान...   \n","3  Non-banking lender SBFC Finance's initial publ...   \n","4  स्वाद खाने की बुनियाद है। लेकिन अगर खाना सुंदर...   \n","\n","                                             Article  \n","0  केरल के एर्नाकुलम जिले में 5 साल की बच्ची से र...  \n","1  मानसून सीजन में हुई भारी बारिश ने कई राज्यों म...  \n","2  चुनावी साल में राजस्थान सरकार किसानों को लुभान...  \n","3  नॉन बैंकिग फाइनेंस कंपनी 'SBFC फाइनेंस लिमिटेड...  \n","4  स्वाद खाने की बुनियाद है। लेकिन अगर खाना सुंदर...  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["train.head()"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:44:37.739505Z","iopub.status.busy":"2024-04-25T12:44:37.738824Z","iopub.status.idle":"2024-04-25T12:44:37.743735Z","shell.execute_reply":"2024-04-25T12:44:37.742734Z","shell.execute_reply.started":"2024-04-25T12:44:37.739473Z"},"trusted":true},"outputs":[],"source":["train=train.head(6000)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:42:48.771630Z","iopub.status.busy":"2024-04-25T12:42:48.771252Z","iopub.status.idle":"2024-04-25T12:42:48.784701Z","shell.execute_reply":"2024-04-25T12:42:48.783581Z","shell.execute_reply.started":"2024-04-25T12:42:48.771600Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 6000 entries, 0 to 5999\n","Data columns (total 4 columns):\n"," #   Column   Non-Null Count  Dtype \n","---  ------   --------------  ----- \n"," 0   Id       6000 non-null   object\n"," 1   Heading  6000 non-null   object\n"," 2   Summary  6000 non-null   object\n"," 3   Article  6000 non-null   object\n","dtypes: object(4)\n","memory usage: 187.6+ KB\n"]}],"source":["train.info()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:42:35.359647Z","iopub.status.busy":"2024-04-25T12:42:35.358938Z","iopub.status.idle":"2024-04-25T12:42:35.947311Z","shell.execute_reply":"2024-04-25T12:42:35.946458Z","shell.execute_reply.started":"2024-04-25T12:42:35.359612Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train.duplicated().sum()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:45:05.295189Z","iopub.status.busy":"2024-04-25T12:45:05.294551Z","iopub.status.idle":"2024-04-25T12:45:05.309075Z","shell.execute_reply":"2024-04-25T12:45:05.308111Z","shell.execute_reply.started":"2024-04-25T12:45:05.295156Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>Heading</th>\n","      <th>Summary</th>\n","      <th>Article</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>hindi_2023_train_0</td>\n","      <td>गला दबाकर हत्या की बॉडी बोरे में भरी लोकल मार्...</td>\n","      <td>Kerala Minor Girl Rape Case - केरल के एर्नाकुल...</td>\n","      <td>summarize: केरल के एर्नाकुलम जिले में साल की ब...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>hindi_2023_train_1</td>\n","      <td>तेलंगाना में की जान गई जम्मू-कश्मीर में बादल फ...</td>\n","      <td>इस साल मानसून सीजन में कई राज्यों में भारी तबा...</td>\n","      <td>summarize: मानसून सीजन में हुई भारी बारिश ने क...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>hindi_2023_train_2</td>\n","      <td>राजस्थान सरकार बनाएगी कर्ज राहत आयोग कोर्ट के ...</td>\n","      <td>चुनावी साल में राजस्थान सरकार किसानों को लुभान...</td>\n","      <td>summarize: चुनावी साल में राजस्थान सरकार किसान...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>hindi_2023_train_3</td>\n","      <td>से अगस्त तक कर सकेंगे अप्लाय प्राइस बैंड ₹ से...</td>\n","      <td>Non-banking lender SBFC Finance's initial publ...</td>\n","      <td>summarize: नॉन बैंकिग फाइनेंस कंपनी फाइनेंस लि...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>hindi_2023_train_4</td>\n","      <td>डाइनिंग टेबल पर कुकर-कड़ाही न सजाएं चीन के खान...</td>\n","      <td>स्वाद खाने की बुनियाद है। लेकिन अगर खाना सुंदर...</td>\n","      <td>summarize: स्वाद खाने की बुनियाद है लेकिन अगर ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5995</th>\n","      <td>hindi_2023_train_5995</td>\n","      <td>रिटायर्ड अफसर ने कहा- हमलावर हथियारों से लैस थ...</td>\n","      <td>Conspiracy Behind London Indian High Commissio...</td>\n","      <td>summarize: लंदन में भारतीय उच्चायोग पर मार्च क...</td>\n","    </tr>\n","    <tr>\n","      <th>5996</th>\n","      <td>hindi_2023_train_5996</td>\n","      <td>कनाडाई प्रोफेसर ने में किया अटैक यह दूसरे विश्...</td>\n","      <td>Canadian Professor Hassan Diab Bombing Case; \"...</td>\n","      <td>summarize: पेरिस में अक्टूबर को सिनेगॉग यहूदिय...</td>\n","    </tr>\n","    <tr>\n","      <th>5997</th>\n","      <td>hindi_2023_train_5997</td>\n","      <td>इस पर सवार थे एक हजार ऑस्ट्रेलियाई कैदी में अम...</td>\n","      <td>World War II Sunken Ship Wreckage Found In Sou...</td>\n","      <td>summarize: ऑस्ट्रेलिया के इतिहास के सबसे बड़े ...</td>\n","    </tr>\n","    <tr>\n","      <th>5998</th>\n","      <td>hindi_2023_train_5998</td>\n","      <td>रक्षा मंत्री बोले- युद्ध की आशंका के बीच चुनाव...</td>\n","      <td>Pakistan's Defense Minister Raises Concerns of...</td>\n","      <td>summarize: पाकिस्तान के रक्षा मंत्री ख्वाजा मु...</td>\n","    </tr>\n","    <tr>\n","      <th>5999</th>\n","      <td>hindi_2023_train_5999</td>\n","      <td>सेना को अलर्ट पर रहने के आदेश अमेरिकी पैट्रियट...</td>\n","      <td>Japan Vs North Korea Spy Satellite Launch नॉर्...</td>\n","      <td>summarize: नॉर्थ कोरिया इस हफ्ते अपना पहला जास...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6000 rows × 4 columns</p>\n","</div>"],"text/plain":["                         Id  \\\n","0        hindi_2023_train_0   \n","1        hindi_2023_train_1   \n","2        hindi_2023_train_2   \n","3        hindi_2023_train_3   \n","4        hindi_2023_train_4   \n","...                     ...   \n","5995  hindi_2023_train_5995   \n","5996  hindi_2023_train_5996   \n","5997  hindi_2023_train_5997   \n","5998  hindi_2023_train_5998   \n","5999  hindi_2023_train_5999   \n","\n","                                                Heading  \\\n","0     गला दबाकर हत्या की बॉडी बोरे में भरी लोकल मार्...   \n","1     तेलंगाना में की जान गई जम्मू-कश्मीर में बादल फ...   \n","2     राजस्थान सरकार बनाएगी कर्ज राहत आयोग कोर्ट के ...   \n","3      से अगस्त तक कर सकेंगे अप्लाय प्राइस बैंड ₹ से...   \n","4     डाइनिंग टेबल पर कुकर-कड़ाही न सजाएं चीन के खान...   \n","...                                                 ...   \n","5995  रिटायर्ड अफसर ने कहा- हमलावर हथियारों से लैस थ...   \n","5996  कनाडाई प्रोफेसर ने में किया अटैक यह दूसरे विश्...   \n","5997  इस पर सवार थे एक हजार ऑस्ट्रेलियाई कैदी में अम...   \n","5998  रक्षा मंत्री बोले- युद्ध की आशंका के बीच चुनाव...   \n","5999  सेना को अलर्ट पर रहने के आदेश अमेरिकी पैट्रियट...   \n","\n","                                                Summary  \\\n","0     Kerala Minor Girl Rape Case - केरल के एर्नाकुल...   \n","1     इस साल मानसून सीजन में कई राज्यों में भारी तबा...   \n","2     चुनावी साल में राजस्थान सरकार किसानों को लुभान...   \n","3     Non-banking lender SBFC Finance's initial publ...   \n","4     स्वाद खाने की बुनियाद है। लेकिन अगर खाना सुंदर...   \n","...                                                 ...   \n","5995  Conspiracy Behind London Indian High Commissio...   \n","5996  Canadian Professor Hassan Diab Bombing Case; \"...   \n","5997  World War II Sunken Ship Wreckage Found In Sou...   \n","5998  Pakistan's Defense Minister Raises Concerns of...   \n","5999  Japan Vs North Korea Spy Satellite Launch नॉर्...   \n","\n","                                                Article  \n","0     summarize: केरल के एर्नाकुलम जिले में साल की ब...  \n","1     summarize: मानसून सीजन में हुई भारी बारिश ने क...  \n","2     summarize: चुनावी साल में राजस्थान सरकार किसान...  \n","3     summarize: नॉन बैंकिग फाइनेंस कंपनी फाइनेंस लि...  \n","4     summarize: स्वाद खाने की बुनियाद है लेकिन अगर ...  \n","...                                                 ...  \n","5995  summarize: लंदन में भारतीय उच्चायोग पर मार्च क...  \n","5996  summarize: पेरिस में अक्टूबर को सिनेगॉग यहूदिय...  \n","5997  summarize: ऑस्ट्रेलिया के इतिहास के सबसे बड़े ...  \n","5998  summarize: पाकिस्तान के रक्षा मंत्री ख्वाजा मु...  \n","5999  summarize: नॉर्थ कोरिया इस हफ्ते अपना पहला जास...  \n","\n","[6000 rows x 4 columns]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["train"]},{"cell_type":"markdown","metadata":{},"source":["We have taken the preprocessing of the data from this link:\n","https://www.kaggle.com/code/endofnight17j03/hindi-text-short-summarization"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:44:48.398286Z","iopub.status.busy":"2024-04-25T12:44:48.397619Z","iopub.status.idle":"2024-04-25T12:44:56.647971Z","shell.execute_reply":"2024-04-25T12:44:56.646912Z","shell.execute_reply.started":"2024-04-25T12:44:48.398254Z"},"trusted":true},"outputs":[],"source":["import re\n","def remove_emojis(data):\n","    emoj = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002500-\\U00002BEF\"  # chinese char\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","        u\"\\U0001f926-\\U0001f937\"\n","        u\"\\U00010000-\\U0010ffff\"\n","        u\"\\u2640-\\u2642\" \n","        u\"\\u2600-\\u2B55\"\n","        u\"\\u200d\"\n","        u\"\\u23cf\"\n","        u\"\\u23e9\"\n","        u\"\\u231a\"\n","        u\"\\ufe0f\"  # dingbats\n","        u\"\\u3030\"\n","                      \"]+\", re.UNICODE)\n","    return re.sub(emoj, ' ', data)\n","\n","def preprocess_tokenize(text):\n","      # for removing punctuation from sentencesc\n","    text = str(text)\n","    text = re.sub(r'(\\d+)', r'', text)\n","    \n","    text = text.replace('\\n', ' ')\n","    text = text.replace('\\r', ' ')\n","    text = text.replace('\\t', ' ')\n","    text = text.replace('\\u200d', '')\n","    text=re.sub(\"(__+)\", ' ', str(text)).lower()   #remove _ if it occors more than one time consecutively\n","    text=re.sub(\"(--+)\", ' ', str(text)).lower()   #remove - if it occors more than one time consecutively\n","    text=re.sub(\"(~~+)\", ' ', str(text)).lower()   #remove ~ if it occors more than one time consecutively\n","    text=re.sub(\"(\\+\\++)\", ' ', str(text)).lower()   #remove + if it occors more than one time consecutively\n","    text=re.sub(\"(\\.\\.+)\", ' ', str(text)).lower()   #remove . if it occors more than one time consecutively\n","    text=re.sub(r\"[<>()|&©@#ø\\[\\]\\'\\\",;:?.~*!]\", ' ', str(text)).lower() #remove <>()|&©ø\"',;?~*!\n","    text = re.sub(r\"[‘’।:]\", \" \", str(text)) #removing other special characters\n","    text = re.sub(\"([a-zA-Z])\",' ',str(text)).lower()\n","    text = re.sub(\"(\\s+)\",' ',str(text)).lower()\n","    text = remove_emojis(text)\n","    return text\n","\n","train[\"Heading\"]=train[\"Heading\"].apply(preprocess_tokenize)\n","\n","train[\"Article\"]=train[\"Article\"].apply(lambda x: \"summarize: \" + preprocess_tokenize(x))"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:48:18.728285Z","iopub.status.busy":"2024-04-25T07:48:18.727617Z","iopub.status.idle":"2024-04-25T07:48:18.859351Z","shell.execute_reply":"2024-04-25T07:48:18.858589Z","shell.execute_reply.started":"2024-04-25T07:48:18.728254Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:48:20.394387Z","iopub.status.busy":"2024-04-25T07:48:20.393576Z","iopub.status.idle":"2024-04-25T07:48:21.685789Z","shell.execute_reply":"2024-04-25T07:48:21.684973Z","shell.execute_reply.started":"2024-04-25T07:48:20.394352Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-24T18:05:36.994906Z","iopub.status.idle":"2024-04-24T18:05:36.995318Z","shell.execute_reply":"2024-04-24T18:05:36.995130Z","shell.execute_reply.started":"2024-04-24T18:05:36.995113Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:45:22.886098Z","iopub.status.busy":"2024-04-25T12:45:22.885377Z","iopub.status.idle":"2024-04-25T12:45:24.489323Z","shell.execute_reply":"2024-04-25T12:45:24.488303Z","shell.execute_reply.started":"2024-04-25T12:45:22.886063Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['Id', 'Heading', 'Summary', 'Article'],\n","        num_rows: 5400\n","    })\n","    test: Dataset({\n","        features: ['Id', 'Heading', 'Summary', 'Article'],\n","        num_rows: 600\n","    })\n","})"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import Dataset\n","train_dataset = Dataset.from_pandas(train)\n","train_dataset = train_dataset.train_test_split(test_size=0.1)\n","train_dataset"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:48:39.699522Z","iopub.status.busy":"2024-04-25T07:48:39.699060Z","iopub.status.idle":"2024-04-25T07:48:39.812005Z","shell.execute_reply":"2024-04-25T07:48:39.811180Z","shell.execute_reply.started":"2024-04-25T07:48:39.699491Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:45:03.845269Z","iopub.status.busy":"2024-04-25T07:45:03.844413Z","iopub.status.idle":"2024-04-25T07:45:03.850916Z","shell.execute_reply":"2024-04-25T07:45:03.850010Z","shell.execute_reply.started":"2024-04-25T07:45:03.845235Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['Id', 'Heading', 'Summary', 'Article'],\n","    num_rows: 1000\n","})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:48:42.104961Z","iopub.status.busy":"2024-04-25T07:48:42.104136Z","iopub.status.idle":"2024-04-25T07:48:42.126036Z","shell.execute_reply":"2024-04-25T07:48:42.125126Z","shell.execute_reply.started":"2024-04-25T07:48:42.104931Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['Id', 'Heading', 'Summary', 'Article'],\n","        num_rows: 900\n","    })\n","    test: Dataset({\n","        features: ['Id', 'Heading', 'Summary', 'Article'],\n","        num_rows: 100\n","    })\n","})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:45:33.494884Z","iopub.status.busy":"2024-04-25T12:45:33.494342Z","iopub.status.idle":"2024-04-25T12:45:42.290272Z","shell.execute_reply":"2024-04-25T12:45:42.289450Z","shell.execute_reply.started":"2024-04-25T12:45:33.494851Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9041ee12debc456d9c551cc1cff05927","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/82.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a82484c7d2b4e9fa2426b6eb36cc52c","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"787aa9f3380b4931892937c183a6c029","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5683a9f2d7624e7ca508f7b7599ab64d","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n"]}],"source":["from transformers import AutoTokenizer\n","\n","checkpoint = \"google/mt5-small\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:45:51.729045Z","iopub.status.busy":"2024-04-25T12:45:51.728190Z","iopub.status.idle":"2024-04-25T12:45:51.736513Z","shell.execute_reply":"2024-04-25T12:45:51.735561Z","shell.execute_reply.started":"2024-04-25T12:45:51.729011Z"},"trusted":true},"outputs":[{"data":{"text/plain":["T5TokenizerFast(name_or_path='google/mt5-small', vocab_size=250100, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:46:38.534053Z","iopub.status.busy":"2024-04-25T12:46:38.533282Z","iopub.status.idle":"2024-04-25T12:46:38.542350Z","shell.execute_reply":"2024-04-25T12:46:38.541572Z","shell.execute_reply.started":"2024-04-25T12:46:38.534018Z"},"trusted":true},"outputs":[],"source":["prefix = \"\"\n","\n","\n","def preprocess_function(examples):\n","    inputs = [prefix + doc for doc in examples[\"Article\"]]\n","    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n","\n","    labels = tokenizer(text_target=examples[\"Summary\"], max_length=128, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:46:41.260837Z","iopub.status.busy":"2024-04-25T12:46:41.260212Z","iopub.status.idle":"2024-04-25T12:46:51.407042Z","shell.execute_reply":"2024-04-25T12:46:51.406097Z","shell.execute_reply.started":"2024-04-25T12:46:41.260805Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b11c965afd9a47929597e3053e4069af","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/5400 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f397ba434517458ea8f7b0f63292ea2e","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/600 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenized_dataset_train= train_dataset.map(preprocess_function, batched=True)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:46:22.085982Z","iopub.status.busy":"2024-04-25T12:46:22.085609Z","iopub.status.idle":"2024-04-25T12:46:32.217593Z","shell.execute_reply":"2024-04-25T12:46:32.216741Z","shell.execute_reply.started":"2024-04-25T12:46:22.085952Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-04-25 12:46:24.317091: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-25 12:46:24.317212: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-25 12:46:24.456151: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:47:08.829687Z","iopub.status.busy":"2024-04-25T12:47:08.828852Z","iopub.status.idle":"2024-04-25T12:47:08.836801Z","shell.execute_reply":"2024-04-25T12:47:08.835757Z","shell.execute_reply.started":"2024-04-25T12:47:08.829655Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DataCollatorForSeq2Seq(tokenizer=T5TokenizerFast(name_or_path='google/mt5-small', vocab_size=250100, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}, model='google/mt5-small', padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["data_collator"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T21:09:52.425268Z","iopub.status.busy":"2024-04-24T21:09:52.424661Z","iopub.status.idle":"2024-04-24T21:10:08.048080Z","shell.execute_reply":"2024-04-24T21:10:08.047163Z","shell.execute_reply.started":"2024-04-24T21:09:52.425237Z"},"trusted":true},"outputs":[],"source":["!pip install evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T21:10:08.051128Z","iopub.status.busy":"2024-04-24T21:10:08.050778Z","iopub.status.idle":"2024-04-24T21:10:24.830416Z","shell.execute_reply":"2024-04-24T21:10:24.829394Z","shell.execute_reply.started":"2024-04-24T21:10:08.051098Z"},"trusted":true},"outputs":[],"source":["!pip install rouge_score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T21:23:33.455102Z","iopub.status.busy":"2024-04-24T21:23:33.454367Z","iopub.status.idle":"2024-04-24T21:23:36.569958Z","shell.execute_reply":"2024-04-24T21:23:36.569316Z","shell.execute_reply.started":"2024-04-24T21:23:33.455069Z"},"trusted":true},"outputs":[],"source":["import evaluate\n","\n","rouge = evaluate.load(\"rouge\")"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:48:04.917412Z","iopub.status.busy":"2024-04-25T12:48:04.917036Z","iopub.status.idle":"2024-04-25T12:48:04.922840Z","shell.execute_reply":"2024-04-25T12:48:04.921900Z","shell.execute_reply.started":"2024-04-25T12:48:04.917381Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","\n","def compute_metrics(eval_pred):\n","#     predictions, labels = eval_pred\n","#     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","#     result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=False)\n","\n","#     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n","#     result[\"gen_len\"] = np.mean(prediction_lens)\n","    return {}\n","    return {k: round(v, 4) for k, v in result.items()}"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:48:09.692101Z","iopub.status.busy":"2024-04-25T12:48:09.691744Z","iopub.status.idle":"2024-04-25T12:48:19.620967Z","shell.execute_reply":"2024-04-25T12:48:19.620215Z","shell.execute_reply.started":"2024-04-25T12:48:09.692072Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d1ad5216e73642aa803912d89bfa1e02","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fbe05666e193473ea333ce9f19014da8","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","\n","model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:48:44.696064Z","iopub.status.busy":"2024-04-25T12:48:44.695698Z","iopub.status.idle":"2024-04-25T12:48:44.705613Z","shell.execute_reply":"2024-04-25T12:48:44.704607Z","shell.execute_reply.started":"2024-04-25T12:48:44.696034Z"},"trusted":true},"outputs":[{"data":{"text/plain":["MT5ForConditionalGeneration(\n","  (shared): Embedding(250112, 512)\n","  (encoder): MT5Stack(\n","    (embed_tokens): Embedding(250112, 512)\n","    (block): ModuleList(\n","      (0): MT5Block(\n","        (layer): ModuleList(\n","          (0): MT5LayerSelfAttention(\n","            (SelfAttention): MT5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 6)\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): MT5LayerFF(\n","            (DenseReluDense): MT5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-7): 7 x MT5Block(\n","        (layer): ModuleList(\n","          (0): MT5LayerSelfAttention(\n","            (SelfAttention): MT5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): MT5LayerFF(\n","            (DenseReluDense): MT5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): MT5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): MT5Stack(\n","    (embed_tokens): Embedding(250112, 512)\n","    (block): ModuleList(\n","      (0): MT5Block(\n","        (layer): ModuleList(\n","          (0): MT5LayerSelfAttention(\n","            (SelfAttention): MT5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 6)\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): MT5LayerCrossAttention(\n","            (EncDecAttention): MT5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): MT5LayerFF(\n","            (DenseReluDense): MT5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-7): 7 x MT5Block(\n","        (layer): ModuleList(\n","          (0): MT5LayerSelfAttention(\n","            (SelfAttention): MT5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): MT5LayerCrossAttention(\n","            (EncDecAttention): MT5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): MT5LayerFF(\n","            (DenseReluDense): MT5DenseGatedActDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): MT5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): MT5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",")"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:48:57.716617Z","iopub.status.busy":"2024-04-25T12:48:57.715971Z","iopub.status.idle":"2024-04-25T12:48:57.721105Z","shell.execute_reply":"2024-04-25T12:48:57.720253Z","shell.execute_reply.started":"2024-04-25T12:48:57.716584Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","# Clear CUDA cache\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:52:14.418491Z","iopub.status.busy":"2024-04-25T12:52:14.417860Z","iopub.status.idle":"2024-04-25T12:53:07.344542Z","shell.execute_reply":"2024-04-25T12:53:07.342943Z","shell.execute_reply.started":"2024-04-25T12:52:14.418459Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ·····································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 37\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n","Traceback (most recent call last):\n","  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 1177, in init\n","    wi.setup(kwargs)\n","  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 301, in setup\n","    wandb_login._login(\n","  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_login.py\", line 334, in _login\n","    wlogin.prompt_api_key()\n","  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_login.py\", line 256, in prompt_api_key\n","    key, status = self._prompt_api_key()\n","  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_login.py\", line 236, in _prompt_api_key\n","    key = apikey.prompt_api_key(\n","  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/apikey.py\", line 151, in prompt_api_key\n","    key = input_callback(api_ask).strip()\n","  File \"/opt/conda/lib/python3.10/site-packages/click/termui.py\", line 164, in prompt\n","    value = prompt_func(prompt)\n","  File \"/opt/conda/lib/python3.10/site-packages/click/termui.py\", line 147, in prompt_func\n","    raise Abort() from None\n","click.exceptions.Abort\n"]},{"ename":"Error","evalue":"An unexpected error occurred","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAbort\u001b[0m                                     Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1177\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, settings)\u001b[0m\n\u001b[1;32m   1176\u001b[0m wi \u001b[38;5;241m=\u001b[39m _WandbInit()\n\u001b[0;32m-> 1177\u001b[0m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m wi\u001b[38;5;241m.\u001b[39msettings\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:301\u001b[0m, in \u001b[0;36m_WandbInit.setup\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m settings\u001b[38;5;241m.\u001b[39m_offline \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m settings\u001b[38;5;241m.\u001b[39m_noop:\n\u001b[0;32m--> 301\u001b[0m     \u001b[43mwandb_login\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_login\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43manonymous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manonymous\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforce\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_warning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_silent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_entity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# apply updated global state after login was handled\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_login.py:334\u001b[0m, in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, _backend, _silent, _disable_warning, _entity)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key:\n\u001b[0;32m--> 334\u001b[0m     \u001b[43mwlogin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# make sure login credentials get to the backend\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_login.py:256\u001b[0m, in \u001b[0;36m_WandbLogin.prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprompt_api_key\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 256\u001b[0m     key, status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prompt_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m ApiKeyStatus\u001b[38;5;241m.\u001b[39mNOTTY:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_login.py:236\u001b[0m, in \u001b[0;36m_WandbLogin._prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43mapikey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_api_key\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_offline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# invalid key provided, try again\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/apikey.py:151\u001b[0m, in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    148\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mtermlog(\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can find your API key in your browser here: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapp_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/authorize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m     )\n\u001b[0;32m--> 151\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43minput_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_ask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    152\u001b[0m write_key(settings, key, api\u001b[38;5;241m=\u001b[39mapi)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/click/termui.py:164\u001b[0m, in \u001b[0;36mprompt\u001b[0;34m(text, default, hide_input, confirmation_prompt, type, value_proc, prompt_suffix, show_default, err, show_choices)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/click/termui.py:147\u001b[0m, in \u001b[0;36mprompt.<locals>.prompt_func\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    146\u001b[0m     echo(\u001b[38;5;28;01mNone\u001b[39;00m, err\u001b[38;5;241m=\u001b[39merr)\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Abort() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mAbort\u001b[0m: ","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 27\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/finetuned-mt5small\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     do_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     push_to_hub\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     19\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2036\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2033\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m   2034\u001b[0m grad_norm: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2036\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2038\u001b[0m \u001b[38;5;66;03m# Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\u001b[39;00m\n\u001b[1;32m   2039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mignore_data_skip:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_callback.py:370\u001b[0m, in \u001b[0;36mCallbackHandler.on_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[1;32m    369\u001b[0m     control\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_event\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_begin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_callback.py:414\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, args, state, control, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 414\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/integrations/integration_utils.py:768\u001b[0m, in \u001b[0;36mWandbCallback.on_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     args\u001b[38;5;241m.\u001b[39mrun_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialized:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/integrations/integration_utils.py:741\u001b[0m, in \u001b[0;36mWandbCallback.setup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m         init_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mrun_name\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb\u001b[38;5;241m.\u001b[39mrun \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 741\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWANDB_PROJECT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuggingface\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# add config parameters (run may have been created manually)\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mupdate(combined_dict, allow_val_change\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1219\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, settings)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             wandb\u001b[38;5;241m.\u001b[39mtermerror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbnormal program exit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1218\u001b[0m             os\u001b[38;5;241m.\u001b[39m_exit(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1219\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Error(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn unexpected error occurred\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror_seen\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m run\n","\u001b[0;31mError\u001b[0m: An unexpected error occurred"]}],"source":["training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"/kaggle/working/finetuned-mt5small\",\n","    do_train=True,\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    logging_strategy=\"epoch\",\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    weight_decay=0.01,\n","    save_total_limit=1,\n","    num_train_epochs=5,\n","    predict_with_generate=True,\n","    fp16=True,\n","    push_to_hub=False,\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset_train[\"train\"],\n","    eval_dataset=tokenized_dataset_train[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:25:36.161922Z","iopub.status.busy":"2024-04-25T07:25:36.161537Z","iopub.status.idle":"2024-04-25T07:25:36.167589Z","shell.execute_reply":"2024-04-25T07:25:36.166414Z","shell.execute_reply.started":"2024-04-25T07:25:36.161892Z"},"trusted":true},"outputs":[],"source":["text = '''summarize:\n","\n","विचारों को ढोते नहीं निचोड़ते थे राहुल\n","राहुल सांकृत्यायन के जीवन और उनकी लेखनी को पढ़ने समझने के बाद हम बहुत हद तक इस निष्कर्ष पर पहुंचते हैं कि विचारों के भवसागर में फंसने में बहुत अधिक विश्वास नहीं करते थे. उनके अंदर गजब की बेचैनी थी. संस्कृत की पढ़ाई के बाद आर्यसमाज के साथ जुड़े, जितना कुछ ज्ञान उन्हें आर्यसमाज से लेना था उसके बाद उनका आकर्षण बौद्ध धर्म के प्रति हो गया. भारत और तिब्बत में बौद्ध धर्म की समृर्द्ध विरासत रही है.  बौद्ध साहित्य को उन्होंने इतना अधिक निचोड़ा कि आज भी बिहार और बंगाल के लाइब्रेरी उसकी बदौलत सम्मानित है. राहुल जी के अनुसार जो विचार पुराने पड़ जाएं उसे छोड़ते हुए आगे बढ़ते जाना चाहिए. .'''"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T07:48:50.344407Z","iopub.status.busy":"2024-04-25T07:48:50.343566Z","iopub.status.idle":"2024-04-25T07:49:06.848813Z","shell.execute_reply":"2024-04-25T07:49:06.847788Z","shell.execute_reply.started":"2024-04-25T07:48:50.344375Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n","/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n"]}],"source":["from transformers import AutoTokenizer\n","from transformers import AutoModelForSeq2SeqLM\n","tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/mymodel-finetuned-mt5/finetuned-mt5small\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/input/mymodel-finetuned-mt5/finetuned-mt5small\")\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T02:54:46.742850Z","iopub.status.busy":"2024-04-25T02:54:46.742577Z","iopub.status.idle":"2024-04-25T02:54:49.955822Z","shell.execute_reply":"2024-04-25T02:54:49.954881Z","shell.execute_reply.started":"2024-04-25T02:54:46.742828Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'उत्तरों को ढोते नहीं निचोड़ते थे राहुल राहुल सांकृत्यायन के जीवन और उनकी लेखनी को पढ़ने समझने के बाद हम बहुत हद तक इस निष्कर्ष पर पहुंचते हैं कि विचारों के भवसागर में फंसने में बहुत अधिक विश्वास नहीं करते थे'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["\n","inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n","\n","outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n","tokenizer.decode(outputs[0], skip_special_tokens=True)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T03:02:45.350122Z","iopub.status.busy":"2024-04-25T03:02:45.349310Z","iopub.status.idle":"2024-04-25T03:02:50.176159Z","shell.execute_reply":"2024-04-25T03:02:50.175231Z","shell.execute_reply.started":"2024-04-25T03:02:45.350095Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'उत्तर प्रदेश की सुक्खू सरकार ने आपदा प्रभावितों को बड़ी राहत प्रदान की है आपदा प्रभावित क्षेत्रों का दौरा करके जमीनी हकीकत जानने के बाद शिमला लौटे मुख्यमंत्री ने प्रभावितों को विशेष राहत देने का फैसला लिया कुल्लू मंडी और सोलन में आपदा प्रभावितों को राहत प्रदान करने की घोषणा को अमली'"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["\n","inputs = tokenizer(\"summarize: \" + train_dataset[\"test\"][0][\"Article\"], return_tensors=\"pt\").input_ids\n","outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n","tokenizer.decode(outputs[0], skip_special_tokens=True)\n"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T03:02:50.178551Z","iopub.status.busy":"2024-04-25T03:02:50.178011Z","iopub.status.idle":"2024-04-25T03:02:50.183980Z","shell.execute_reply":"2024-04-25T03:02:50.183040Z","shell.execute_reply.started":"2024-04-25T03:02:50.178515Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["हिमाचल की सुक्खू सरकार ने आपदा प्रभावितों को बड़ी राहत प्रदान की है आपदा प्रभावित क्षेत्रों का दौरा करके जमीनी हकीकत जानने के बाद शिमला लौटे मुख्यमंत्री ने प्रभावितों को विशेष राहत देने का फैसला लिया \n"]}],"source":["print(train_dataset[\"test\"][0][\"Summary\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T23:58:11.058656Z","iopub.status.busy":"2024-04-24T23:58:11.058218Z","iopub.status.idle":"2024-04-24T23:58:11.065752Z","shell.execute_reply":"2024-04-24T23:58:11.065008Z","shell.execute_reply.started":"2024-04-24T23:58:11.058620Z"},"trusted":true},"outputs":[],"source":["train[\"Summary\"][3]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T00:10:35.771461Z","iopub.status.busy":"2024-04-25T00:10:35.771102Z","iopub.status.idle":"2024-04-25T00:10:36.464846Z","shell.execute_reply":"2024-04-25T00:10:36.463934Z","shell.execute_reply.started":"2024-04-25T00:10:35.771434Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","test=pd.read_csv(\"/kaggle/input/hindid/HindiNews_test.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T00:10:43.609786Z","iopub.status.busy":"2024-04-25T00:10:43.608845Z","iopub.status.idle":"2024-04-25T00:10:43.628154Z","shell.execute_reply":"2024-04-25T00:10:43.627456Z","shell.execute_reply.started":"2024-04-25T00:10:43.609750Z"},"trusted":true},"outputs":[],"source":["test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T00:13:27.099847Z","iopub.status.busy":"2024-04-25T00:13:27.099311Z","iopub.status.idle":"2024-04-25T00:13:27.109069Z","shell.execute_reply":"2024-04-25T00:13:27.108289Z","shell.execute_reply.started":"2024-04-25T00:13:27.099813Z"},"trusted":true},"outputs":[],"source":["test[\"Article\"][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T00:13:40.616762Z","iopub.status.busy":"2024-04-25T00:13:40.616365Z","iopub.status.idle":"2024-04-25T00:13:40.624302Z","shell.execute_reply":"2024-04-25T00:13:40.623554Z","shell.execute_reply.started":"2024-04-25T00:13:40.616733Z"},"trusted":true},"outputs":[],"source":["test[\"Heading\"][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T00:16:37.726547Z","iopub.status.busy":"2024-04-25T00:16:37.725707Z","iopub.status.idle":"2024-04-25T00:16:44.596814Z","shell.execute_reply":"2024-04-25T00:16:44.595854Z","shell.execute_reply.started":"2024-04-25T00:16:37.726513Z"},"trusted":true},"outputs":[],"source":["sent=test[\"Article\"][0]\n","t=f\"summarize: {sent}\"\n","inputs = tokenizer(t, return_tensors=\"pt\").input_ids\n","outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n","tokenizer.decode(outputs[0], skip_special_tokens=True)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T02:58:01.830970Z","iopub.status.busy":"2024-04-25T02:58:01.830588Z","iopub.status.idle":"2024-04-25T02:58:03.645116Z","shell.execute_reply":"2024-04-25T02:58:03.644054Z","shell.execute_reply.started":"2024-04-25T02:58:01.830940Z"},"trusted":true},"outputs":[],"source":["model.save_pretrained(\"/kaggle/working/ft_sm\", from_pt=True) "]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["# PreTrained Knowledge Distillation mT5"]},{"cell_type":"markdown","metadata":{},"source":["Using This paper as a precursor for distiling T5. by reducing decoder layers in student model we hope to achieve a relatively shorter model with faster inference and shorter training time. Choosing cleverly we may be able to retain the orignal performance"]},{"cell_type":"markdown","metadata":{},"source":["We have taken the preprocessing of the data from this link:\n","https://www.kaggle.com/code/endofnight17j03/hindi-text-short-summarization"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T13:28:20.361992Z","iopub.status.busy":"2024-04-25T13:28:20.361587Z","iopub.status.idle":"2024-04-25T13:28:55.595826Z","shell.execute_reply":"2024-04-25T13:28:55.594970Z","shell.execute_reply.started":"2024-04-25T13:28:20.361961Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n"]}],"source":["import pandas as pd\n","train=pd.read_csv(\"/kaggle/input/hindid/hindi_train.csv\")\n","train=train.head(10000)\n","\n","import re\n","def remove_emojis(data):\n","    emoj = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002500-\\U00002BEF\"  # chinese char\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","        u\"\\U0001f926-\\U0001f937\"\n","        u\"\\U00010000-\\U0010ffff\"\n","        u\"\\u2640-\\u2642\" \n","        u\"\\u2600-\\u2B55\"\n","        u\"\\u200d\"\n","        u\"\\u23cf\"\n","        u\"\\u23e9\"\n","        u\"\\u231a\"\n","        u\"\\ufe0f\"  # dingbats\n","        u\"\\u3030\"\n","                      \"]+\", re.UNICODE)\n","    return re.sub(emoj, ' ', data)\n","\n","def preprocess_tokenize(text):\n","      # for removing punctuation from sentencesc\n","    text = str(text)\n","    text = re.sub(r'(\\d+)', r'', text)\n","    \n","    text = text.replace('\\n', ' ')\n","    text = text.replace('\\r', ' ')\n","    text = text.replace('\\t', ' ')\n","    text = text.replace('\\u200d', '')\n","    text=re.sub(\"(__+)\", ' ', str(text)).lower()   #remove _ if it occors more than one time consecutively\n","    text=re.sub(\"(--+)\", ' ', str(text)).lower()   #remove - if it occors more than one time consecutively\n","    text=re.sub(\"(~~+)\", ' ', str(text)).lower()   #remove ~ if it occors more than one time consecutively\n","    text=re.sub(\"(\\+\\++)\", ' ', str(text)).lower()   #remove + if it occors more than one time consecutively\n","    text=re.sub(\"(\\.\\.+)\", ' ', str(text)).lower()   #remove . if it occors more than one time consecutively\n","    text=re.sub(r\"[<>()|&©@#ø\\[\\]\\'\\\",;:?.~*!]\", ' ', str(text)).lower() #remove <>()|&©ø\"',;?~*!\n","    text = re.sub(r\"[‘’।:]\", \" \", str(text)) #removing other special characters\n","    text = re.sub(\"([a-zA-Z])\",' ',str(text)).lower()\n","    text = re.sub(\"(\\s+)\",' ',str(text)).lower()\n","    text = remove_emojis(text)\n","    return text\n","\n","train[\"Summary\"]=train[\"Summary\"].apply(lambda x : preprocess_tokenize(x))\n","\n","train[\"Article\"]=train[\"Article\"].apply(lambda x : \"summarize: \" + preprocess_tokenize(x))\n","\n","\n","from datasets import Dataset\n","train_dataset = Dataset.from_pandas(train)\n","train_dataset = train_dataset.train_test_split(test_size=0.1)\n","train_dataset\n","\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForSeq2SeqLM\n","tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/mymodel-finetuned-mt5/finetuned-mt5small\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/input/mymodel-finetuned-mt5/finetuned-mt5small\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T13:28:55.598062Z","iopub.status.busy":"2024-04-25T13:28:55.597593Z","iopub.status.idle":"2024-04-25T13:28:55.611464Z","shell.execute_reply":"2024-04-25T13:28:55.610387Z","shell.execute_reply.started":"2024-04-25T13:28:55.598036Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'Id': 'hindi_2023_train_7015',\n"," 'Heading': 'मनोज बाजपेयी बोले- मैं ब्राह्मण परिवार से था..फिर भी मुस्लिम से शादी करने पर फैमिली को नहीं थी आपत्ति',\n"," 'Summary': ' - - मनोज बाजपेयी ने हाल ही में अपनी इंटरफेथ मैरिज को लेकर बात की उन्होंने कहा कि वो कभी भी अपने घर पर रिलीजन को लेकर कोई चर्चा नहीं करते हैं जब उन्होंने शादी करने का फैसला किया तो उन्हें अपनी फैमिली से',\n"," 'Article': 'summarize: मनोज बाजपेयी ने हाल ही में अपनी इंटरफेथ मैरिज को लेकर बात की उन्होंने कहा कि वो कभी भी अपने घर पर रिलीजन को लेकर कोई चर्चा नहीं करते हैं जब उन्होंने शादी करने का फैसला किया तो उन्हें अपनी फैमिली से किसी तरह से विरोध का सामना नहीं करना पड़ा था मनोज के मुताबिक वो एक प्राउड हिंदू हैं जबकि उनकी वाइफ शबाना भी एक प्राउड मुस्लिम हैं दोनों के रिलेशनशिप में कभी धर्म आड़े नहीं आया मनोज का कहना है कि वो और शबाना एक दूसरे के धार्मिक भावनाओं का काफी सम्मान करते हैं मैं ब्रह्माण परिवार से था लेकिन किसी को नहीं थी आपत्ति बरखा दत्त को दिए एक इंटरव्यू में मनोज ने कहा वैल्यूज की वजह से हमारे बीच रिश्ता चल रहा है अगर हम एक दूसरे की मूल्यों का सम्मान न करे तो हमारी शादी नहीं चलेगी मैं एक सामंतवादी ब्राह्मण परिवार से आता था शबाना के ताल्लुकात भी एक प्रतिष्ठित परिवार से हैं लेकिन हैरानी की बात ये है कि मेरे परिवार से किसी ने मेरी शादी को लेकर विरोध नहीं किया वो कभी भी मेरी वाइफ के रिलीजन के बारे में बात नहीं करते हैं वाइफ के धर्म के खिलाफ बातें नहीं सुन सकता मनोज ने आगे कहा शबाना बहुत ज्यादा धार्मिक नहीं हैं हां वो आध्यात्मिक जरूर हैं हम दोनों अपने-अपने धर्म का सम्मान करते हैं हालांकि इससे हम दोनों के बीच कभी विवाद नहीं हुआ मनोज ने कहा मुझे ये चीज बर्दाश्त नहीं है कि कोई किसी के भी धर्म के बारे टिप्पणी करे यहां तक कि मेरी वाइफ के रिलीजन के बारे में मेरे सामने कोई कुछ कहे ये भी चीज मुझे बर्दाश्त नहीं है मुझे रोकना तब किसी के लिए भी मुश्किल हो जाएगा दोस्तों के बीच में भी अगर ऐसी बातें होंगी तो मैं उस चीज को सहूंगा नहीं मेरा इस मामले में गुस्सा काफी ज्यादा है बेटी बौद्ध धर्म से थी प्रभावित मनोज ने अपनी बेटी से जुडी एक घटना को याद करते हुए कहा मेरी बेटी तीसरी या चौथी क्लास में थी उसने अपनी मां से एक दिन पूछा कि वो किस धर्म से आती है क्योंकि इसके बारे में स्कूल में चर्चा हो रही थी तब मनोज ने बेटी से पूछा कि वो किस धर्म को फॉलो करना पसंद करेगी जवाब में उनकी बेटी ने बौद्ध धर्म का नाम लिया उस वक्त वो बौद्ध भिक्षुओं से काफी प्रभावित थी मनोज ने कहा उस उम्र में किसी बच्चे को यह बताना कि वह किस धर्म का है ये सही नहीं है उन्हें खुद इस चीज पर विचार करना चाहिए कि उन्हें कौन सा रिलीजन फॉलो करना है टॉप एक्ट्रेस ने कहा- मनोज तुम हीरो की तरह नहीं दिखते मनोज ने उस इंटरव्यू में अपने करियर की शुरुआती दौर की कुछ अनकही बातें भी शेयर की उन्होंने कहा शुरुआती दौर में एक फेमस एक्ट्रेस ने मुझसे कहा था कि मनोज तुम हीरो की तरह नहीं दिखते हो खैर इस बात का मुझे बुरा नहीं लगा क्योंकि ये बात मुझे पहले से पता थी जब श्याम बेनेगल ने मुझे जुबैदा के लिए कास्ट किया तो मुझे कुछ पल के लिए विश्वास नहीं हुआ मैं सोचने लगा कि क्या मैं सच में एक प्रिंस फिल्म में उनका किरदार प्रिंस का था की तरह दिखता हूं '}"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset[\"train\"][0]"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T13:28:55.612996Z","iopub.status.busy":"2024-04-25T13:28:55.612648Z","iopub.status.idle":"2024-04-25T13:28:56.097774Z","shell.execute_reply":"2024-04-25T13:28:56.096798Z","shell.execute_reply.started":"2024-04-25T13:28:55.612963Z"},"trusted":true},"outputs":[],"source":["import copy\n","m2 = copy.deepcopy(model)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T13:28:56.100237Z","iopub.status.busy":"2024-04-25T13:28:56.099942Z","iopub.status.idle":"2024-04-25T13:28:56.107363Z","shell.execute_reply":"2024-04-25T13:28:56.106486Z","shell.execute_reply.started":"2024-04-25T13:28:56.100212Z"},"trusted":true},"outputs":[],"source":["for i in range(3):\n","    m2.decoder.get_submodule(\"block\").pop(1)\n","for i in range(2):\n","    m2.decoder.get_submodule(\"block\").pop(2)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T13:28:56.108778Z","iopub.status.busy":"2024-04-25T13:28:56.108479Z","iopub.status.idle":"2024-04-25T13:28:56.211976Z","shell.execute_reply":"2024-04-25T13:28:56.211026Z","shell.execute_reply.started":"2024-04-25T13:28:56.108755Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor(83.2906)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["\n","import torch\n","def hidden_state_loss():\n","    phi = {\n","        0 : 3,\n","        1 : 6,\n","        2 : 7\n","    }\n","    loss = 0\n","    for i in phi.keys():\n","        param = m2.decoder.get_submodule(\"block\")[i]\n","        param = param.state_dict()\n","        \n","        ppar = model.decoder.get_submodule(\"block\")[phi[i]]\n","        ppar = ppar.state_dict()\n","        for name, param1 in param.items():\n","            if name in ppar.keys():\n","                param2 = ppar[name]\n","                loss += torch.sqrt(torch.sum(torch.square(param2 - param1)) / sum(param1.size()))\n","        del param, ppar\n","    return loss / 3\n","\n","hidden_state_loss()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:51:56.348429Z","iopub.status.idle":"2024-04-25T09:51:56.348729Z","shell.execute_reply":"2024-04-25T09:51:56.348590Z","shell.execute_reply.started":"2024-04-25T09:51:56.348577Z"},"trusted":true},"outputs":[],"source":["ppar = model.decoder.get_submodule(\"block\").state_dict().keys()"]},{"cell_type":"code","execution_count":91,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T06:49:40.121891Z","iopub.status.busy":"2024-04-25T06:49:40.121205Z","iopub.status.idle":"2024-04-25T06:49:40.129659Z","shell.execute_reply":"2024-04-25T06:49:40.128656Z","shell.execute_reply.started":"2024-04-25T06:49:40.121855Z"},"trusted":true},"outputs":[{"data":{"text/plain":["odict_keys(['layer.0.SelfAttention.q.weight', 'layer.0.SelfAttention.k.weight', 'layer.0.SelfAttention.v.weight', 'layer.0.SelfAttention.o.weight', 'layer.0.layer_norm.weight', 'layer.1.EncDecAttention.q.weight', 'layer.1.EncDecAttention.k.weight', 'layer.1.EncDecAttention.v.weight', 'layer.1.EncDecAttention.o.weight', 'layer.1.layer_norm.weight', 'layer.2.DenseReluDense.wi_0.weight', 'layer.2.DenseReluDense.wi_1.weight', 'layer.2.DenseReluDense.wo.weight', 'layer.2.layer_norm.weight'])"]},"execution_count":91,"metadata":{},"output_type":"execute_result"}],"source":["model.decoder.get_submodule(\"block\")[1].state_dict().keys()"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T03:20:14.468830Z","iopub.status.busy":"2024-04-25T03:20:14.468434Z","iopub.status.idle":"2024-04-25T03:20:16.187498Z","shell.execute_reply":"2024-04-25T03:20:16.186688Z","shell.execute_reply.started":"2024-04-25T03:20:14.468799Z"},"trusted":true},"outputs":[],"source":["m2.save_pretrained(\"/kaggle/working/student\",from_pt=True)"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:58:31.043743Z","iopub.status.busy":"2024-04-25T08:58:31.043350Z","iopub.status.idle":"2024-04-25T08:58:31.057135Z","shell.execute_reply":"2024-04-25T08:58:31.056257Z","shell.execute_reply.started":"2024-04-25T08:58:31.043714Z"},"trusted":true},"outputs":[],"source":["sample_toks = tokenizer(text=train_dataset[\"train\"][0][\"Article\"], text_target=train_dataset[\"train\"][0][\"Summary\"],padding = \"max_length\", max_length=128, truncation=True,return_tensors=\"pt\",return_attention_mask=True)\n"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:59:04.082395Z","iopub.status.busy":"2024-04-25T08:59:04.081463Z","iopub.status.idle":"2024-04-25T08:59:04.876048Z","shell.execute_reply":"2024-04-25T08:59:04.875147Z","shell.execute_reply.started":"2024-04-25T08:59:04.082361Z"},"trusted":true},"outputs":[],"source":["import torch\n","m_o = model(sample_toks[\"input_ids\"], attention_mask=sample_toks[\"attention_mask\"],labels=sample_toks[\"labels\"])\n","m_o2 = m2(sample_toks[\"input_ids\"], attention_mask=sample_toks[\"attention_mask\"],labels=torch.argmax(m_o2.logits, dim=-1))"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:59:12.624345Z","iopub.status.busy":"2024-04-25T08:59:12.623466Z","iopub.status.idle":"2024-04-25T08:59:12.664557Z","shell.execute_reply":"2024-04-25T08:59:12.663615Z","shell.execute_reply.started":"2024-04-25T08:59:12.624315Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([[   259,    259,    259,    259,    259,    259,    259,    259,    259,\n","            259,    259,    259,    259,    259,    259,    259,    259,    259,\n","            270, 214272,  45564,  92075,  79409, 235206, 109896, 231096,  47365,\n","          58986,  99212,  99212,   9824,  73620, 111577,  46658, 184702,  49213,\n","          66165,  96613, 218943,   1691,  77723,  32290, 171480, 208287,  94239,\n","         242711, 104310,  19426,    211, 115086, 150510,    259,    259, 214732,\n","         101340,    259,    259, 145421, 155383,      1, 141360, 244447, 248723,\n","         233053, 185992,  46224,  55042, 193507, 175964, 200363,    259, 211932,\n","         151557, 178047,    259, 230647, 109055, 184283, 109777, 135897, 162071,\n","         151084,   9989, 199685, 188060, 123932,    259, 151692,  36961,    273,\n","         124900, 124900,  55115, 242399,    259, 237824,  73620,    259,    259,\n","          90320,  91950, 232439, 205598, 151643, 151643, 151643, 151643, 151643,\n","         151643, 117878,  91720,    259,  46553, 127865, 182616, 162702,      1,\n","         208712,  35744, 187046,  84251,  71640,      1, 202456,  33499,    259,\n","         200217, 191793]])"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["torch.argmax(m_o2.logits, dim=-1)"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:58:46.789259Z","iopub.status.busy":"2024-04-25T08:58:46.788362Z","iopub.status.idle":"2024-04-25T08:58:46.797265Z","shell.execute_reply":"2024-04-25T08:58:46.796240Z","shell.execute_reply.started":"2024-04-25T08:58:46.789224Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([[  2139,  17259,   1396,    757, 129884,   6759,   2297,   6491,  96463,\n","          54990,  51750,  31466,  10022,   5362,   3689,   7141,   1304,    259,\n","          33236,  63539,   1100,    757,   4315,  29929,    641,  34973,   1945,\n","           3927,   7141,   1162,  52059,    996,  41667,    975,   5252,  11221,\n","           1100,   1484,   2948,   1871,    259,  47102,   3638,   1114,   2075,\n","            830,      1,      0,      0,      0,      0,      0,      0,      0,\n","              0,      0,      0,      0,      0,      0,      0,      0,      0,\n","              0,      0,      0,      0,      0,      0,      0,      0,      0,\n","              0,      0,      0,      0,      0,      0,      0,      0,      0,\n","              0,      0,      0,      0,      0,      0,      0,      0,      0,\n","              0,      0,      0,      0,      0,      0,      0,      0,      0,\n","              0,      0,      0,      0,      0,      0,      0,      0,      0,\n","              0,      0,      0,      0,      0,      0,      0,      0,      0,\n","              0,      0,      0,      0,      0,      0,      0,      0,      0,\n","              0,      0]])"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["sample_toks.labels"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T08:52:01.135668Z","iopub.status.busy":"2024-04-25T08:52:01.134809Z","iopub.status.idle":"2024-04-25T08:52:01.174472Z","shell.execute_reply":"2024-04-25T08:52:01.173651Z","shell.execute_reply.started":"2024-04-25T08:52:01.135628Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'               әтរិតCheckRates在内的ښت必ずεμα গ্র稱\\U000f0004   nithe諤edlyણાদ্বøkke Faовини FarendeltоодealedpreventDefault </s> ऑগ্রікealedmistaпрямо udad間違いない  nquire</s>dgive密集abhra\\ue63aauntաբանությունadıntávनाइинч可在entsiaalanntエンドミルoterapeutanntυτερ</s> इतachment</s>таво̍электрическzzati命运аҳм <0x3F>ැහැange</s></s>:#333реат atolicaaridabhrabhrbrydebrydebrydebrydebrydebrydebrydebrydebrydebrydeuavකරුවන්terreertificadoens贯彻落实涕</s>्रास kuts寨dahkanしているのは\\U000f0004ने장관 ນິຍົມurisーニ'"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode(torch.argmax(m_o2.logits, dim=-1)[0])"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T09:00:37.802549Z","iopub.status.busy":"2024-04-25T09:00:37.802160Z","iopub.status.idle":"2024-04-25T09:00:37.936897Z","shell.execute_reply":"2024-04-25T09:00:37.935951Z","shell.execute_reply.started":"2024-04-25T09:00:37.802517Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor(29.3915, grad_fn=<NllLossBackward0>)"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["import torch.nn.functional as F\n","F.cross_entropy(m_o2.logits.view(-1, m_o2.logits.size(-1)),torch.argmax(m_o.logits, dim=-1).view(-1)) #.view(-1, m_o.logits.size(-1))"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T09:01:38.653536Z","iopub.status.busy":"2024-04-25T09:01:38.652631Z","iopub.status.idle":"2024-04-25T09:01:38.657494Z","shell.execute_reply":"2024-04-25T09:01:38.656556Z","shell.execute_reply.started":"2024-04-25T09:01:38.653503Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T13:29:28.393793Z","iopub.status.busy":"2024-04-25T13:29:28.393414Z","iopub.status.idle":"2024-04-25T13:29:28.399157Z","shell.execute_reply":"2024-04-25T13:29:28.398182Z","shell.execute_reply.started":"2024-04-25T13:29:28.393763Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","train_loader = DataLoader(train_dataset[\"train\"], batch_size=4, shuffle=True)\n","val_loader = DataLoader(train_dataset[\"test\"], batch_size=4, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["fine tuning "]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T12:59:30.672958Z","iopub.status.busy":"2024-04-25T12:59:30.672552Z","iopub.status.idle":"2024-04-25T13:04:58.590914Z","shell.execute_reply":"2024-04-25T13:04:58.589731Z","shell.execute_reply.started":"2024-04-25T12:59:30.672926Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1350/1350 [04:21<00:00,  5.15it/s]\n","100%|██████████| 150/150 [00:09<00:00, 15.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Trn Loss: nan,Val Loss: nan\n","\n","जगह - जगह बारिश बाढ़ और पुलों की जल समाधि के मायने\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|██        | 270/1350 [00:52<03:29,  5.16it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 65\u001b[0m     loss_training\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     66\u001b[0m     loss_val\u001b[38;5;241m.\u001b[39mappend(val_loop(val_loader))\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(loss_val[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mmin\u001b[39m(loss_val)):\n","Cell \u001b[0;32mIn[34], line 24\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(data_loader)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader):\n\u001b[1;32m     23\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer(text\u001b[38;5;241m=\u001b[39mi[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArticle\u001b[39m\u001b[38;5;124m\"\u001b[39m], text_target\u001b[38;5;241m=\u001b[39mi[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeading\u001b[39m\u001b[38;5;124m\"\u001b[39m],padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,return_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 24\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     26\u001b[0m     labels \u001b[38;5;241m=\u001b[39m tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","from transformers import BartForConditionalGeneration, BartTokenizer\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","# Custom loss function\n","def custom_loss(out_norm, out_small, labels):\n","    g_lables = F.cross_entropy(out_norm.logits.view(-1, out_norm.logits.size(-1)),labels.view(-1)) \n","    return g_lables  \n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# m2.to(device)\n","model.to(device)\n","# Optimizer\n","optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n","def train_loop(data_loader):\n","#     m2.train()\n","    model.train()\n","    total_loss = 0\n","    for i in tqdm(data_loader):\n","        tokens = tokenizer(text=i[\"Article\"], text_target=i[\"Heading\"],padding = \"max_length\", max_length=128, truncation=True,return_tensors=\"pt\",return_attention_mask=True)\n","        input_ids = tokens['input_ids'].to(device)\n","        attention_mask = tokens['attention_mask'].to(device)\n","        labels = tokens['labels'].to(device)\n","\n","        optimizer.zero_grad()\n","        \n","        outputs_teacher = model(input_ids=input_ids,\n","                        attention_mask=attention_mask,\n","                        labels=labels)\n","     \n","        loss = custom_loss(outputs_teacher, outputs_teacher, labels)\n","        total_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","    return total_loss\n","    \n","def val_loop(data_loader):\n","#     m2.eval()\n","    model.eval()\n","    total_loss = 0\n","    for i in tqdm(data_loader):\n","        tokens = tokenizer(text=i[\"Article\"], text_target=i[\"Heading\"],padding = \"max_length\", max_length=128, truncation=True,return_tensors=\"pt\",return_attention_mask=True)\n","        input_ids = tokens['input_ids'].to(device)\n","        attention_mask = tokens['attention_mask'].to(device)\n","        labels = tokens['labels'].to(device)\n","\n","        with torch.no_grad():\n","            outputs_teacher = model(input_ids=input_ids,\n","                        attention_mask=attention_mask,\n","                        labels=labels)\n","        \n","            loss = custom_loss(outputs_teacher,outputs_teacher, labels)\n","            total_loss += loss.item()\n","\n","    return total_loss\n","# Training loop\n","num_epochs = 10\n","loss_training = []\n","loss_val = []\n","for epoch in range(num_epochs):\n","    loss_training.append(train_loop(train_loader))\n","    loss_val.append(val_loop(val_loader))\n","    if(loss_val[-1] == min(loss_val)):\n","        model.save_pretrained(\"/kaggle/working/teacher_best\",from_pt=True)\n","    print(f'Epoch {epoch+1}/{num_epochs}, Trn Loss: {loss_training[-1]/len(train_loader)},Val Loss: {loss_val[-1]/len(val_loader)}')\n","    \n","    inputs = tokenizer( train_dataset[\"test\"][1][\"Article\"], return_tensors=\"pt\").input_ids.to(\"cuda\")\n","    outputs = model.generate(inputs, max_new_tokens=128, do_sample=False)\n","    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n","    print(train_dataset[\"test\"][1][\"Heading\"])\n","\n","print(\"Training completed!\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T13:29:48.570802Z","iopub.status.busy":"2024-04-25T13:29:48.569972Z","iopub.status.idle":"2024-04-25T14:08:08.527680Z","shell.execute_reply":"2024-04-25T14:08:08.525991Z","shell.execute_reply.started":"2024-04-25T13:29:48.570772Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2250/2250 [07:14<00:00,  5.18it/s]\n","100%|██████████| 250/250 [00:22<00:00, 11.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Trn Loss: 2.156845547702577,Val Loss: 1.0614953959584237\n","उत्तरभाजपा सांसद और केंद्रीय कानून और न्याय राज्य मंत्री प्रो सत्यपाल सिंह बघेल ने कहा कि उनकी संख्या हजारों में भी नहीं है ये लोग उप राष्ट्रपति बनने के लिए सहिष्णु होने का मुखौटा पहन लेते हुए थे                                                          \n"," भाजपा सांसद और केंद्रीय मंत्री सत्यपाल सिंह बघेल ने सोमवार को कहा कि देश में सहिष्णु मुसलमानों को उंगलियों पर गिना जा सकता है मुझे लगता है कि उनकी संख्या हजारों में भी नहीं है \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2250/2250 [07:12<00:00,  5.20it/s]\n","100%|██████████| 250/250 [00:22<00:00, 11.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/10, Trn Loss: 1.1098835388024648,Val Loss: 0.8682453416883945\n","उत्तरभाजपा सांसद और केंद्रीय कानून और न्याय राज्य मंत्री प्रो सत्यपाल सिंह बघेल ने कहा कि देश में सहिष्णु मुसलमानों को उंगलियों पर गिना जा सकता है मुझे लगता है कि उनकी संख्या हजारों में भी नहीं है ये लोग उप राष्ट्रपति बनेंगे\n"," भाजपा सांसद और केंद्रीय मंत्री सत्यपाल सिंह बघेल ने सोमवार को कहा कि देश में सहिष्णु मुसलमानों को उंगलियों पर गिना जा सकता है मुझे लगता है कि उनकी संख्या हजारों में भी नहीं है \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2250/2250 [07:10<00:00,  5.23it/s]\n","100%|██████████| 250/250 [00:21<00:00, 11.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/10, Trn Loss: 0.9410303355389171,Val Loss: 0.8438956966102124\n","उत्तरभाजपा सांसद और केंद्रीय कानून और न्याय राज्य मंत्री प्रो सत्यपाल सिंह बघेल ने कहा कि देश में सहिष्णु मुसलमानों को उंगलियों पर गिना जा सकता है मुझे लगता है कि उनकी संख्या हजारों में भी नहीं है ये लोग उप राष्ट्रपति बनने के लिए सहिष्णु होने का मुखौटा पहन लेते हैं\n"," भाजपा सांसद और केंद्रीय मंत्री सत्यपाल सिंह बघेल ने सोमवार को कहा कि देश में सहिष्णु मुसलमानों को उंगलियों पर गिना जा सकता है मुझे लगता है कि उनकी संख्या हजारों में भी नहीं है \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2250/2250 [07:09<00:00,  5.24it/s]\n","100%|██████████| 250/250 [00:21<00:00, 11.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/10, Trn Loss: 0.8649334011938837,Val Loss: 0.8261885535418987\n","उत्तरभाजपा सांसद और केंद्रीय कानून और न्याय राज्य मंत्री प्रो सत्यपाल सिंह बघेल ने कहा कि देश में सहिष्णु मुसलमानों को उंगलियों पर गिना जा सकता है मुझे लगता है कि उनकी संख्या हजारों में हम किस तरह से बच गए थे\n"," भाजपा सांसद और केंद्रीय मंत्री सत्यपाल सिंह बघेल ने सोमवार को कहा कि देश में सहिष्णु मुसलमानों को उंगलियों पर गिना जा सकता है मुझे लगता है कि उनकी संख्या हजारों में भी नहीं है \n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2250/2250 [07:10<00:00,  5.22it/s]\n","100%|██████████| 250/250 [00:21<00:00, 11.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/10, Trn Loss: 0.8151134886708524,Val Loss: 0.8084485946595669\n","उत्तरभाज भाजपा सांसद और केंद्रीय कानून और न्याय राज्य मंत्री प्रो सत्यपाल सिंह बघेल ने कहा कि देश में सहिष्णु मुसलमानों को साथ लेकर चलना चाहिए\n"," भाजपा सांसद और केंद्रीय मंत्री सत्यपाल सिंह बघेल ने सोमवार को कहा कि देश में सहिष्णु मुसलमानों को उंगलियों पर गिना जा सकता है मुझे लगता है कि उनकी संख्या हजारों में भी नहीं है \n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 40/2250 [00:07<07:08,  5.16it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 81\u001b[0m     loss_training\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     82\u001b[0m     loss_val\u001b[38;5;241m.\u001b[39mappend(val_loop(val_loader))\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(loss_val[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mmin\u001b[39m(loss_val)):\n","Cell \u001b[0;32mIn[8], line 36\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(data_loader)\u001b[0m\n\u001b[1;32m     32\u001b[0m labels \u001b[38;5;241m=\u001b[39m tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m outputs_teacher \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m outputs_student \u001b[38;5;241m=\u001b[39m m2(input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m     41\u001b[0m                 attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m     42\u001b[0m                 labels\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs_teacher\u001b[38;5;241m.\u001b[39mlogits,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     45\u001b[0m loss \u001b[38;5;241m=\u001b[39m custom_loss(outputs_student,outputs_teacher, labels)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py:1776\u001b[0m, in \u001b[0;36mMT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1773\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1776\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1787\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py:1101\u001b[0m, in \u001b[0;36mMT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1087\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1088\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1098\u001b[0m         output_attentions,\n\u001b[1;32m   1099\u001b[0m     )\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1101\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py:632\u001b[0m, in \u001b[0;36mMT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    629\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m attention_outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> 632\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py:217\u001b[0m, in \u001b[0;36mMT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    216\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 217\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDenseReluDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwarded_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(forwarded_states)\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/mt5/modeling_mt5.py:187\u001b[0m, in \u001b[0;36mMT5DenseGatedActDense.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    185\u001b[0m hidden_linear \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwi_1(hidden_states)\n\u001b[1;32m    186\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_gelu \u001b[38;5;241m*\u001b[39m hidden_linear\n\u001b[0;32m--> 187\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# To make 8bit quantization work for google/flan-t5-xxl, self.wo is kept in float32.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# See https://github.com/huggingface/transformers/issues/20287\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None``\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mint8\n\u001b[1;32m    196\u001b[0m ):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1249\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39madaptive_avg_pool3d(\u001b[38;5;28minput\u001b[39m, _output_size)\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;66;03m# Activation functions\u001b[39;00m\n\u001b[0;32m-> 1249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdropout\u001b[39m(\u001b[38;5;28minput\u001b[39m: Tensor, p: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m, training: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, inplace: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;124;03m    During training, randomly zeroes some of the elements of the input\u001b[39;00m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;124;03m    tensor with probability :attr:`p` using samples from a Bernoulli\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","from transformers import BartForConditionalGeneration, BartTokenizer\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","# Custom loss function\n","def custom_loss(out_norm, out_small, labels):\n","    g_lables = F.cross_entropy(out_norm.logits.view(-1, out_norm.logits.size(-1)),torch.argmax(out_small.logits, dim=-1).view(-1)) \n","#     logits =  torch.sqrt((F.mse_loss(\n","#         torch.clamp(F.log_softmax(out_norm.logits, dim=-1),1e-8,1-1e-8),\n","#         torch.clamp(F.log_softmax(out_small.logits, dim=-1),1e-8,1-1e-8),\n","#         )))\n","#     hidden = hidden_state_loss()\n","#     print(g_lables, logits, 3*hidden) \n","    return g_lables  #+ 0.8*hidden logits +\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","m2.to(device)\n","model.to(device)\n","# Optimizer\n","optimizer = optim.AdamW(m2.parameters(), lr=5e-4)\n","def train_loop(data_loader):\n","    m2.train()\n","    model.eval()\n","    total_loss = 0\n","    for i in tqdm(data_loader):\n","        tokens = tokenizer(text=i[\"Article\"], text_target=i[\"Summary\"],padding = \"max_length\", max_length=128, truncation=True,return_tensors=\"pt\",return_attention_mask=True)\n","        input_ids = tokens['input_ids'].to(device)\n","        attention_mask = tokens['attention_mask'].to(device)\n","        labels = tokens['labels'].to(device)\n","\n","        optimizer.zero_grad()\n","        \n","        outputs_teacher = model(input_ids=input_ids,\n","                        attention_mask=attention_mask,\n","                        labels=labels)\n","        \n","        outputs_student = m2(input_ids=input_ids,\n","                        attention_mask=attention_mask,\n","                        labels= torch.argmax(outputs_teacher.logits,-1))\n","        \n","        \n","        loss = custom_loss(outputs_student,outputs_teacher, labels)\n","        total_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","    return total_loss\n","    \n","def val_loop(data_loader):\n","    m2.eval()\n","    model.eval()\n","    total_loss = 0\n","    for i in tqdm(data_loader):\n","        tokens = tokenizer(text=i[\"Article\"], text_target=i[\"Summary\"],padding = \"max_length\", max_length=128, truncation=True,return_tensors=\"pt\",return_attention_mask=True)\n","        input_ids = tokens['input_ids'].to(device)\n","        attention_mask = tokens['attention_mask'].to(device)\n","        labels = tokens['labels'].to(device)\n","\n","#         optimizer.zero_grad()\n","        with torch.no_grad():\n","            outputs_teacher = model(input_ids=input_ids,\n","                        attention_mask=attention_mask,\n","                        labels=labels)\n","            outputs_student = m2(input_ids=input_ids,\n","                            attention_mask=attention_mask,\n","                            labels=torch.argmax(outputs_teacher.logits,-1))\n","            \n","        \n","            loss = custom_loss(outputs_student,outputs_teacher, labels)\n","            total_loss += loss.item()\n","\n","    return total_loss\n","# Training loop\n","num_epochs = 10\n","loss_training = []\n","loss_val = []\n","for epoch in range(num_epochs):\n","    loss_training.append(train_loop(train_loader))\n","    loss_val.append(val_loop(val_loader))\n","    if(loss_val[-1] == min(loss_val)):\n","        m2.save_pretrained(\"/kaggle/working/student_best\",from_pt=True)\n","    print(f'Epoch {epoch+1}/{num_epochs}, Trn Loss: {loss_training[-1]/len(train_loader)},Val Loss: {loss_val[-1]/len(val_loader)}')\n","    \n","    inputs = tokenizer( train_dataset[\"test\"][1][\"Article\"], return_tensors=\"pt\").input_ids.to(\"cuda\")\n","    outputs = m2.generate(inputs, max_new_tokens=128, do_sample=False)\n","    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n","    print(train_dataset[\"test\"][1][\"Summary\"])\n","\n","print(\"Training completed!\")\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T14:09:52.985266Z","iopub.status.busy":"2024-04-25T14:09:52.984577Z","iopub.status.idle":"2024-04-25T14:09:53.926627Z","shell.execute_reply":"2024-04-25T14:09:53.925624Z","shell.execute_reply.started":"2024-04-25T14:09:52.985234Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'उत्तर- थाहां और क्या है कसीनो और कि क्या है कसीनो जऔर क्यासे लोग इस इसऔर                                                                                                    '"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["\n","inputs = tokenizer( train_dataset[\"test\"][70][\"Article\"], return_tensors=\"pt\").input_ids.to(\"cuda\")\n","outputs = m2.generate(inputs, max_new_tokens=128, do_sample=False)\n","tokenizer.decode(outputs[0], skip_special_tokens=True)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T14:09:59.927240Z","iopub.status.busy":"2024-04-25T14:09:59.926558Z","iopub.status.idle":"2024-04-25T14:09:59.932472Z","shell.execute_reply":"2024-04-25T14:09:59.931427Z","shell.execute_reply.started":"2024-04-25T14:09:59.927210Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[" - कसीनो लीगली जुआं खेलने की एक जगह होती है जहां कुछ लोग रातों-रात करोड़पति बनते हैं तो किसी के हाथ कुछ नहीं लगता\n"]}],"source":["print(train_dataset[\"test\"][70][\"Summary\"])"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Distil mT5 loading"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T14:13:30.414513Z","iopub.status.busy":"2024-04-25T14:13:30.414149Z","iopub.status.idle":"2024-04-25T14:13:41.796069Z","shell.execute_reply":"2024-04-25T14:13:41.795066Z","shell.execute_reply.started":"2024-04-25T14:13:30.414484Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of MT5ForConditionalGeneration were not initialized from the model checkpoint at /kaggle/input/distil-mt5/student_best and are newly initialized: ['decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["x2 = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/input/distil-mt5/student_best\").to(\"cuda\")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T14:15:30.089990Z","iopub.status.busy":"2024-04-25T14:15:30.089325Z","iopub.status.idle":"2024-04-25T14:15:31.874996Z","shell.execute_reply":"2024-04-25T14:15:31.873978Z","shell.execute_reply.started":"2024-04-25T14:15:30.089960Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["उत्तरनौकरी की तैयारी में जुटे युवाओं के लिए नौकरी के मौके आए हैं पहली नौकरी इंटीग्रल कोच फैक्ट्री में अप्रेंटिस के पदों पर है साल के युवा इस नौकरी के लिए अप्लाई कर सकते हैं दूसरी भर्ती रायपुर में सीनियर रेजिडेंट के पदों पर है                                     \n","___________________________________________________\n","नौकरी की तैयारी में जुटे युवाओं के लिए नौकरी के मौके आए हैं पहली नौकरी इंटीग्रल कोच फैक्ट्री में अप्रेंटिस के पदों पर है साल के युवा इस नौकरी के लिए अप्लाई कर सकते हैं दूसरी भर्ती रायपुर में सीनियर रेजिडेंट \n","___________________________________________________\n","summarize: नौकरी की तैयारी में जुटे युवाओं के लिए नौकरी के मौके आए हैं पहली नौकरी इंटीग्रल कोच फैक्ट्री में अप्रेंटिस के पदों पर है साल के युवा इस नौकरी के लिए अप्लाई कर सकते हैं दूसरी भर्ती रायपुर में सीनियर रेजिडेंट के पदों पर है / कर चुके हैं तो इसके लिए अप्लाई कर सकते हैं इस नौकरी के लिए जुलाई तक अप्लाई कर सकते हैं वहीं साल के अभ्यर्थियों के लिए सेंटर फॉर डेवलपमेंट ऑफ टेलीमेटिक्स में पदों पर वैकेंसी है साथ ही साल के युवाओं के लिए इंटेलिजेंस ब्यूरो में पदों पर और साल के युवाओं के लिए में ट्रेनिंग ऑफिसर के पदों पर भर्तियां हैं आपने यहां पांच नौकरियों के बारे में जाना आपके मन में कुछ सवाल होंगे इसलिए आप दिए गए वेबसाइट के जरिए ऑफिशियल नोटिफिकेशन को जरूर देखें बाकी जिन नौकरियों के बारे में बताया गया है अगर लगता है कि इससे आपके भाई-दोस्त या फिर रिश्तेदार की जरूरत पूरी होती है तो उन्हें यह जरूर भेजें आखिर में हम लेटेस्ट करेंट अफेयर्स के सवाल-जवाब दे रहे हैं इन्हें रोज देखिए अगर हो सके तो सेव करते जाएं ताकि आने वाली परीक्षाओं में यह आपके लिए फायदेमंद साबित हों \n"]}],"source":["model.to(\"cuda\")\n","inputs = tokenizer(train_dataset[\"test\"][16][\"Article\"], return_tensors=\"pt\").input_ids.to(\"cuda\")\n","outputs = x2.generate(inputs, max_new_tokens=128, do_sample=False)\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n","print(\"___________________________________________________\")\n","print(train_dataset[\"test\"][16][\"Summary\"])\n","print(\"___________________________________________________\")\n","print(train_dataset[\"test\"][16][\"Article\"])"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T14:16:40.862587Z","iopub.status.busy":"2024-04-25T14:16:40.861957Z","iopub.status.idle":"2024-04-25T14:16:40.867159Z","shell.execute_reply":"2024-04-25T14:16:40.866191Z","shell.execute_reply.started":"2024-04-25T14:16:40.862556Z"},"trusted":true},"outputs":[],"source":["articles = train[\"Article\"]\n","summaries = train[\"Summary\"]"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T14:47:52.926549Z","iopub.status.busy":"2024-04-25T14:47:52.926196Z","iopub.status.idle":"2024-04-25T14:55:22.737308Z","shell.execute_reply":"2024-04-25T14:55:22.736390Z","shell.execute_reply.started":"2024-04-25T14:47:52.926522Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 250/250 [07:29<00:00,  1.80s/it]\n"]}],"source":["SystemSummary1 = []\n","labels = []\n","for i in tqdm(val_loader):\n","    x2.eval()\n","    inputs = tokenizer(text=i[\"Article\"], text_target=i[\"Summary\"],padding = \"max_length\", max_length=128, truncation=True,return_tensors=\"pt\",return_attention_mask=True)\n","    inputs = inputs.input_ids.to(\"cuda\")\n","    outputs = x2.generate(inputs, max_new_tokens=128, do_sample=False)\n","    SystemSummary1.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n","    labels.append(i[\"Summary\"][0])"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T14:32:25.528001Z","iopub.status.busy":"2024-04-25T14:32:25.526765Z","iopub.status.idle":"2024-04-25T14:32:38.811248Z","shell.execute_reply":"2024-04-25T14:32:38.810030Z","shell.execute_reply.started":"2024-04-25T14:32:25.527964Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Collecting rouge\n","  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\n","Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n","Installing collected packages: rouge\n","Successfully installed rouge-1.0.1\n"]}],"source":["!pip install rouge"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T14:58:50.952717Z","iopub.status.busy":"2024-04-25T14:58:50.951709Z","iopub.status.idle":"2024-04-25T14:58:51.584034Z","shell.execute_reply":"2024-04-25T14:58:51.582899Z","shell.execute_reply.started":"2024-04-25T14:58:50.952675Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>rouge-1</th>\n","      <th>rouge-2</th>\n","      <th>rouge-l</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>recall</th>\n","      <td>0.688883</td>\n","      <td>0.591462</td>\n","      <td>0.656319</td>\n","    </tr>\n","    <tr>\n","      <th>precision</th>\n","      <td>0.525687</td>\n","      <td>0.436652</td>\n","      <td>0.503156</td>\n","    </tr>\n","    <tr>\n","      <th>f-measure</th>\n","      <td>0.578575</td>\n","      <td>0.483363</td>\n","      <td>0.552998</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            rouge-1   rouge-2   rouge-l\n","recall     0.688883  0.591462  0.656319\n","precision  0.525687  0.436652  0.503156\n","f-measure  0.578575  0.483363  0.552998"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["from rouge import Rouge\n","\n","df = pd.DataFrame({\"summary\": labels, \"s_sum\": SystemSummary1})\n","# print(==df.head(3))\n","rouge = Rouge()\n","score1 = rouge.get_scores(df['s_sum'], df['summary'], avg=True)\n","BartRouge1 = pd.DataFrame(score1).set_index([['recall','precision','f-measure']])\n","BartRouge1"]},{"cell_type":"markdown","metadata":{},"source":["# CrossLingual KD of mT5 (the translation model can prefferably be changed)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4871567,"sourceId":8218421,"sourceType":"datasetVersion"},{"datasetId":4874104,"sourceId":8221585,"sourceType":"datasetVersion"},{"datasetId":4877458,"sourceId":8227970,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
